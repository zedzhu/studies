# 机器学习损失函数
通常，监督学习的优化目标函数（或者叫代价函数）形式如下[^1]：
$$
\theta^* = \arg \min_\theta (\frac{1}{N}\sum_{i=1}^{N} L(y_i, f(x_i; \theta)) + \lambda\  \Omega(\theta) )
$$
即学习到使得右边式子达到最小值的参数$\theta$的解$\theta^*$。其中，前面部分$L(y_i, f(x_i; \theta))$是第 *i* 个样本的预测值$f(x_i; \theta)$和真实值$y_i$之间的误差，也即损失函数。后面的$\Omega(\theta)$是对参数$\theta$的正则化项或者惩罚项，主要用来降低模型的复杂度，避免过拟合。损失函数 *L* 也被称为经验风险，加上正则项后就是结构风险。

## 分类问题
### 1. 0-1损失
$$
L(y, f(x))=\left\{
\begin{aligned}
1, \quad y \neq f(x)\\
0, \quad y=f(x)
\end{aligned}
\right.
$$
### 2. Hinge损失

### 3. Logistic损失(log loss)

### 4. 交叉熵损失(Cross Entropy)

### 5. softmax loss

## 回归问题
### 1、平方损失(Mean Square Error)

### 2、Huber损失

# 深度学习激活函数
### 1、对数几率Sigmoid

### 2、双曲正切Tanh

### 3、修正线性单元ReLu

[^1]: http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/  
[^2]: https://blog.csdn.net/zouxy09/article/details/24971995
